"""
### Overview
Source file: **dags/example_dag.py**

Purpose of this DAG is to demonstrate a simple pipeline consisting of multiple tasks in Airflow. It illustrates a basic workflow where each task performs a specific action, and dependencies between tasks are established to ensure proper execution order.

### Input
- Input data files or resources required by individual tasks should be provided in the respective task definitions or via Airflow variables.

### Output

- Output generated by tasks in this pipeline could include processed data, logs, or any other artifacts produced during task execution.

### Variables
The variables of this DAG are stored in Airflow variable named ***<DAG_name>***, which contains the following key-value pairs:
- ***input_data_path***: Path to the input data directory.
- ***output_data_path***: Path to the output data directory.
- ***email_on_failure***: Email address to receive notifications in case of DAG failure.
- ***email_on_success***: Email address to receive notifications upon successful DAG completion.

### Connections
This DAG needs following connections to run properly:
- ***<connection_name>***: <use_case_description>

Sample values for variable:
```
{
    "input_data_path": "/path/to/input/data",
    "output_data_path": "/path/to/output/data",
    "email_on_failure": "xyz@abc.com",
    "email_on_success": "xyz@abc.com"
}
```

### Notes
While operating this DAG, there's a few things to keep in mind:
- Ensure that 'input_data_path' and 'output_data_path' variables are properly configured to point to the correct directories containing input data and where output should be saved.
- Set appropriate email addresses for 'email_on_failure' and 'email_on_success' variables to receive notifications as per your requirement.
- Monitor task execution logs and airflow UI to ensure proper execution of tasks and troubleshoot any issues that may arise during DAG execution.
"""
import logging
import datetime
from airflow.decorators import task, dag

__dag_name__ = "example_DAG"
__description__ = "Just an example DAG"
__owner__ = 'minhpc@ikameglobal.com'
__schedule__ = None  # Cron string to schedule the DAG, should be put to DAG variable to avoid hard-coding.
__start_date__ = datetime.datetime(2024, 1, 1)
__tags__ = ["example"]


@task()
def prepare_data(from_value: int, to_value: int, **kwargs) -> list[int]:
    """
    Task to prepare data for processing.

    Args:
        from_value: Start value for data preparation.
        to_value: End value for data preparation.

    Returns:
        List of batches of integers.
    """

    list_of_int = list(range(from_value, to_value + 1, 1))
    # Split into 10 chunks
    chunk_size = len(list_of_int) // 10
    chunks = [list_of_int[i:i + chunk_size] for i in range(0, len(list_of_int), chunk_size)]

    return chunks


@task(queue='testing_queue')
def process_data(data: list[int], **kwargs) -> int:
    """
    Task to process data.

    Args:
        data: List of integers to process.

    Returns:
        Sum of all integers in the input list.
    """
    return sum(data)


@task(queue='testing_queue')
def aggregate_result(**kwargs) -> None:
    """
    Task to aggregate results from multiple tasks.

    Args:
        kwargs: Context passed by Airflow.

    Returns:
        Sum of all integers processed by tasks in the pipeline.
    """
    from_value = kwargs['params']['from_value']
    to_value = kwargs['params']['to_value']

    list_task_result = kwargs['ti'].xcom_pull(task_ids="process_data")

    logging.info(f'Sum of integers from {from_value} to {to_value} is: {sum(list_task_result)}')


@dag(
    catchup=False,
    dag_id=__dag_name__,
    default_args={
        'owner': __owner__,
        'email': [__owner__],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 0,
        'retry_delay': datetime.timedelta(seconds=60),
        'max_active_tis_per_dag': 8,
    },
    description=__description__,
    doc_md=__doc__,
    params={
        "from_value": 1,
        "to_value": 100
    },
    render_template_as_native_obj=True,
    schedule=__schedule__,
    start_date=__start_date__,
    tags=__tags__,
)
def generate_dag():
    _prepare_data = prepare_data(from_value="{{ dag_run.conf.from_value }}", to_value="{{ dag_run.conf.to_value }}")
    _process_data = process_data.expand(data=_prepare_data)
    _aggregate_result = aggregate_result()

    _prepare_data >> _process_data >> _aggregate_result


dag_instance = generate_dag()

if __name__ == "__main__":
    dag_instance.test(
        run_conf={
            "from_value": 1,
            "to_value": 1000
        }
    )
